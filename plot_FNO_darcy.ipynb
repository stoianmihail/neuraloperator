{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Training a neural operator on Darcy-Flow\n",
        "In this example, we demonstrate how to use the small Darcy-Flow example we ship with the package\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "from neuralop.models import TFNO\n",
        "from neuralop import Trainer\n",
        "from neuralop.datasets import load_darcy_flow_small\n",
        "from neuralop.utils import count_params\n",
        "from neuralop import LpLoss, H1Loss\n",
        "\n",
        "device = 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading the Navier-Stokes dataset in 128x128 resolution\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_loader, test_loaders, output_encoder = load_darcy_flow_small(\n",
        "        n_train=1000, batch_size=32, \n",
        "        test_resolutions=[16, 32], n_tests=[100, 50],\n",
        "        test_batch_sizes=[32, 32],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create a tensorized FNO model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = TFNO(n_modes=(16, 16), hidden_channels=32, projection_channels=64, factorization='tucker', rank=0.42)\n",
        "model = model.to(device)\n",
        "\n",
        "n_params = count_params(model)\n",
        "print(f'\\nOur model has {n_params} parameters.')\n",
        "sys.stdout.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create the optimizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), \n",
        "                                lr=8e-3, \n",
        "                                weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating the losses\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "l2loss = LpLoss(d=2, p=2)\n",
        "h1loss = H1Loss(d=2)\n",
        "\n",
        "train_loss = h1loss\n",
        "eval_losses={'h1': h1loss, 'l2': l2loss}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print('\\n### MODEL ###\\n', model)\n",
        "print('\\n### OPTIMIZER ###\\n', optimizer)\n",
        "print('\\n### SCHEDULER ###\\n', scheduler)\n",
        "print('\\n### LOSSES ###')\n",
        "print(f'\\n * Train: {train_loss}')\n",
        "print(f'\\n * Test: {eval_losses}')\n",
        "sys.stdout.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create the trainer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(model, n_epochs=20,\n",
        "                  device=device,\n",
        "                  mg_patching_levels=0,\n",
        "                  wandb_log=False,\n",
        "                  log_test_interval=3,\n",
        "                  use_distributed=False,\n",
        "                  verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Actually train the model on our small Darcy-Flow dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer.train(train_loader, test_loaders,\n",
        "              output_encoder,\n",
        "              model, \n",
        "              optimizer,\n",
        "              scheduler, \n",
        "              regularizer=False, \n",
        "              training_loss=train_loss,\n",
        "              eval_losses=eval_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the prediction, and compare with the ground-truth \n",
        "Note that we trained on a very small resolution for\n",
        "a very small number of epochs\n",
        "In practice, we would train at larger resolution, on many more samples.\n",
        "\n",
        "However, for practicity, we created a minimal example that\n",
        "i) fits in just a few Mb of memory\n",
        "ii) can be trained quickly on CPU\n",
        "\n",
        "In practice we would train a Neural Operator on one or multiple GPUs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "test_samples = test_loaders[32].dataset\n",
        "\n",
        "fig = plt.figure(figsize=(7, 7))\n",
        "for index in range(3):\n",
        "    data = test_samples[index]\n",
        "    # Input x\n",
        "    x = data['x']\n",
        "    # Ground-truth\n",
        "    y = data['y']\n",
        "    # Model prediction\n",
        "    out = model(x.unsqueeze(0))\n",
        "\n",
        "    ax = fig.add_subplot(3, 3, index*3 + 1)\n",
        "    ax.imshow(x[0], cmap='gray')\n",
        "    if index == 0: \n",
        "        ax.set_title('Input x')\n",
        "    plt.xticks([], [])\n",
        "    plt.yticks([], [])\n",
        "\n",
        "    ax = fig.add_subplot(3, 3, index*3 + 2)\n",
        "    ax.imshow(y.squeeze())\n",
        "    if index == 0: \n",
        "        ax.set_title('Ground-truth y')\n",
        "    plt.xticks([], [])\n",
        "    plt.yticks([], [])\n",
        "\n",
        "    ax = fig.add_subplot(3, 3, index*3 + 3)\n",
        "    ax.imshow(out.squeeze().detach().numpy())\n",
        "    if index == 0: \n",
        "        ax.set_title('Model prediction')\n",
        "    plt.xticks([], [])\n",
        "    plt.yticks([], [])\n",
        "\n",
        "fig.suptitle('Inputs, ground-truth output and prediction.', y=0.98)\n",
        "plt.tight_layout()\n",
        "fig.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
